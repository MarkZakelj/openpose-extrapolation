{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from src.paths import DATA_DIR\n",
    "\n",
    "VERSION = 'v8'\n",
    "random.seed(42)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "NECK = 1\n",
    "def scale_pose(poses, scale):\n",
    "    poses = poses.clone()\n",
    "    meanx = poses[:, :, 0].mean(dim=1)\n",
    "    meany = poses[:, :, 1].mean(dim=1)\n",
    "    poses[:, :, 0] -= meanx.unsqueeze(1)\n",
    "    poses[:, :, 1] -= meany.unsqueeze(1)\n",
    "    poses[:, :, 0] *= scale[0]\n",
    "    poses[:, :, 1] *= scale[1]\n",
    "    poses[:, :, 0] += meanx.unsqueeze(1)\n",
    "    poses[:, :, 1] += meany.unsqueeze(1)\n",
    "    return poses\n",
    "\n",
    "def calculate_relative_points(poses):\n",
    "    res = poses.clone()\n",
    "    res -= poses[:, NECK:NECK+1, :] # subtract neck\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(DATA_DIR, 'poses.csv'))\n",
    "df = df[~(df['class'].isin(['squat']))] # remove squats\n",
    "df = df.drop('class', axis=1) # remove class column\n",
    "df = df[~(df == 0).any(axis=1)] # remove rows with any zeros - undefined ponints\n",
    "poses = torch.tensor(df.values, dtype=torch.float32) # convert to tensor\n",
    "poses = rearrange(poses, 'n (ps p) -> n ps p', p=2) # rearrange to (n, 18, 2)\n",
    "poses = scale_pose(poses, (1.75, 1.0)) # fix x-axis points\n",
    "poses = calculate_relative_points(poses) # change to relative to NECK\n",
    "\n",
    "# augument the dataset with y-axis flipping\n",
    "p1 = poses.clone()\n",
    "p1[:, :, 0] *= -1\n",
    "poses_flipped = torch.cat([poses, p1], dim=0)\n",
    "\n",
    "# augument the dataset with scaling from 0.05 to 3.55\n",
    "get_scale = lambda : (random.betavariate(alpha=0.9, beta=1.1)) * 3.5 + 0.05\n",
    "REPETITIONS = 2\n",
    "res = []\n",
    "for n in range(REPETITIONS):\n",
    "    p1 = poses_flipped.clone()\n",
    "    scale = torch.tensor([get_scale() for _ in range(p1.shape[0])], dtype=torch.float32).unsqueeze(1).unsqueeze(2)\n",
    "    p1 *= scale\n",
    "    res.append(p1)\n",
    "poses_scaled = torch.cat(res, dim=0)\n",
    "\n",
    "# augument the dataset with x and y axis rotation up to +- 45 degrees\n",
    "# this matrix represents y-axis rotation followed by x-axis rotation\n",
    "def rotate_both_axis(thetax: float, thetay: float):\n",
    "    t = [\n",
    "        [math.cos(thetay), math.sin(thetay) * math.sin(thetax)],\n",
    "        [0, math.cos(thetax)]\n",
    "    ]\n",
    "    return t\n",
    "ROTATION_REPETITIONS = 2\n",
    "get_rotationy = lambda: (random.betavariate(alpha=2.5, beta=2.5) - 0.5) * np.pi / 2\n",
    "get_rotationx = lambda: (random.betavariate(alpha=7, beta=7) - 0.5) * np.pi / 2\n",
    "res = []\n",
    "for n in range(ROTATION_REPETITIONS):\n",
    "    matrices = torch.tensor([rotate_both_axis(get_rotationx(), get_rotationy()) for _ in range(poses_scaled.shape[0])], dtype=torch.float32)\n",
    "    p1 = poses_scaled.clone()\n",
    "    p1 = torch.matmul(p1, matrices)\n",
    "    res.append(p1)\n",
    "poses_scaled = torch.cat(res, dim=0)\n",
    "\n",
    "# augument dataset based on x-scaling\n",
    "get_scale = lambda : (random.betavariate(alpha=2, beta=2)) + 0.5\n",
    "REPETITIONS = 1\n",
    "res = []\n",
    "for n in range(REPETITIONS):\n",
    "    p1 = poses_scaled.clone()\n",
    "    scale = torch.tensor([get_scale() for _ in range(p1.shape[0])], dtype=torch.float32).reshape(-1, 1)\n",
    "    p1[:, :, 0] *= scale\n",
    "    res.append(p1)\n",
    "poses_scaled = torch.cat(res, dim=0)\n",
    "\n",
    "# # add image ratios\n",
    "nposes = poses_scaled.shape[0]\n",
    "ratios = [1344/768, 1.0, 768/1344]\n",
    "res_scaled = []\n",
    "for ratio in ratios:\n",
    "    p1 = poses_scaled.clone()\n",
    "    p1[:, :, 0] *= ratio\n",
    "    res_scaled.append(p1)\n",
    "poses_scaled = torch.cat(res_scaled, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPETITIONS = 30\n",
    "res = []\n",
    "res_missing = []\n",
    "for n in range(REPETITIONS):\n",
    "    p1 = poses_scaled.clone()\n",
    "    p2 = poses_scaled.clone()\n",
    "    res.append(p2)\n",
    "    bool_tensor = torch.zeros(p1.shape, dtype=torch.bool)\n",
    "    num_false_per_row = torch.randint(1, 13, (p1.shape[0],))\n",
    "    for i, num_false in enumerate(num_false_per_row):\n",
    "        false_indices = torch.randperm(18)[:num_false]\n",
    "        bool_tensor[i, false_indices, :] = True\n",
    "    p1[bool_tensor] = -10\n",
    "    res_missing.append(p1)\n",
    "poses_missing = torch.cat(res_missing, dim=0)\n",
    "poses_scaled = torch.cat(res, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange from (n, 18, 2) to (n, 36)\n",
    "poses_scaled_rearranged = rearrange(poses_scaled, 'n ps p -> n (ps p)')\n",
    "poses_missing_rearranged = rearrange(poses_missing, 'n ps p -> n (ps p)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1060560, 36]) torch.Size([1060560, 36])\n"
     ]
    }
   ],
   "source": [
    "# check if ratios and points are of the same length and \n",
    "print(poses_scaled_rearranged.shape, poses_missing_rearranged.shape)\n",
    "assert poses_scaled_rearranged.shape[0] == poses_missing_rearranged.shape[0]\n",
    "assert poses_scaled_rearranged.shape[1] == poses_missing_rearranged.shape[1] == 36\n",
    "# if missing and scaled are the same except for the missing points\n",
    "mask = poses_missing_rearranged != -10.0\n",
    "assert (poses_missing_rearranged[mask] == poses_scaled_rearranged[mask]).all()\n",
    "\n",
    "data_dir = Path(DATA_DIR, VERSION)\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# split into train, validation and test\n",
    "total_size = poses_missing_rearranged.shape[0]\n",
    "train_size = int(0.7 * total_size)\n",
    "valid_size = int(0.15 * total_size)\n",
    "\n",
    "# Create indices for train, validation, test\n",
    "indices = torch.randperm(total_size).tolist()\n",
    "train_indices = indices[:train_size]\n",
    "valid_indices = indices[train_size:train_size+valid_size]\n",
    "test_indices = indices[train_size+valid_size:]\n",
    "\n",
    "# save shuffled data\n",
    "torch.save(poses_scaled_rearranged[train_indices], Path(data_dir, 'poses_train.pt'))\n",
    "torch.save(poses_scaled_rearranged[valid_indices], Path(data_dir, 'poses_valid.pt'))\n",
    "torch.save(poses_scaled_rearranged[test_indices], Path(data_dir, 'poses_test.pt'))\n",
    "torch.save(poses_missing_rearranged[train_indices], Path(data_dir, 'poses_missing_train.pt'))\n",
    "torch.save(poses_missing_rearranged[valid_indices], Path(data_dir, 'poses_missing_valid.pt'))\n",
    "torch.save(poses_missing_rearranged[test_indices], Path(data_dir, 'poses_missing_test.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
